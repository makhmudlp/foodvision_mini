{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa3fa9e",
   "metadata": {},
   "source": [
    "# Machine Learning Model Deployment\n",
    "\n",
    "Machine learning model deployment involves making your model available to someone or something else. For example, someone might use your model as part of a food recognition app (such as FoodVision Mini or [Nutrify](https://nutrify.app)). And something else might be another model or program using your model such as a banking system using a machine learning model to detect if a transaction is fraud or not.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from going_modular import data_setup, utils\n",
    "from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device =\"cpu\" # 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af787e",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "For our problem we will be using 20% of the pizza, steak and sushi data from Food101\n",
    "\n",
    "I can download the data using the `download_data()` function I created in helper_function.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ca1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_20_percent_path=download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
    "data_20_percent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = data_20_percent_path / \"train\"\n",
    "test_dir = data_20_percent_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769eb89",
   "metadata": {},
   "source": [
    "## 2. FoodVision Mini model deployment experiment\n",
    "\n",
    "The ideal deployed model FoodVision Mini performs well and fast. \n",
    "\n",
    "We'd like our model to perform as close to real-time as possible.\n",
    "\n",
    "So our goals are:\n",
    "\n",
    "1. **Performance** - A model that performs at 95%+ accuracy.\n",
    "2. **Speed** - A model that can classify an image at ~30FPS (0.03 seconds inference time per image, also known as latency)\n",
    "\n",
    "To try and achieve these results, let's bring in our best performing models from the previous sections: \n",
    "\n",
    "1. **EffNetB2 feature extractor** (EffNetB2 for short) - with adjusted `classifier` layers.\n",
    "2. **ViT-B/16 feature extractor** (ViT for short) -  with adjusted `head` layers.\n",
    "    * **Note** ViT-B/16 stands for \"Vision Transformer Base, patch size 16\".\n",
    "\n",
    "**Note:** A \"feature extractor model\" often starts with a model that has been pretrained on a dataset similar to your own problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da75fbdc",
   "metadata": {},
   "source": [
    "### Creating EffNetB2 feature extractor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab17e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup pretrained EffNetB2 weights\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "# 2. Get EffNetB2 transforms\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "# 3. Setup pretrained model\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
    "\n",
    "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd08a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out EffNetB2 classifier head\n",
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abacae6d",
   "metadata": {},
   "source": [
    "#### Change the classifier\n",
    "\n",
    "Now we need to change out features of our classifier layer because we are classifyin 3 types of food: sushi, steak, and pizza. EffNetB2 default classifier arhitecture is for 1000 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92267725",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
    "    nn.Linear(in_features=1408, # keep in_features same \n",
    "              out_features=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c872e2",
   "metadata": {},
   "source": [
    "#### Functionizing creating effnetb2\n",
    "Let's make a function to make it easier to create effnetb2 model in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
    "                                                      seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d743e",
   "metadata": {},
   "source": [
    "#### Creating dataloaders for effnetb2\n",
    "\n",
    "Now we need create data_loaders for test and train. To do this we will call data_setup.create_dataloader() function that is available at going modular directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import data_setup\n",
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloader(train_dir=train_dir,\n",
    "                                                                                                 test_dir=test_dir,\n",
    "                                                                                                 transform=effnetb2_transforms,\n",
    "                                                                                                 batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747673c3",
   "metadata": {},
   "source": [
    "#### Training EffNetB2 feature extractor\n",
    "\n",
    "Model ready, `DataLoader`s ready, let's train!\n",
    "\n",
    "Ten epochs should be enough to get good results.\n",
    "\n",
    "We can do so by creating an optimizer (we'll use `torch.optim.Adam()` with a learning rate of `1e-3`), a loss function (we'll use `torch.nn.CrossEntropyLoss()`) for multi-class classification) and then passing these as well as our `DataLoader`s to the [`engine.train()`] function we created in goind_modular directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1366c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Setup loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set seeds for reproducibility and train the model\n",
    "set_seeds()\n",
    "effnetb2_results = engine.train(model=effnetb2,\n",
    "                                train_dataloader=train_dataloader_effnetb2,\n",
    "                                test_dataloader=test_dataloader_effnetb2,\n",
    "                                epochs=10,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe896a",
   "metadata": {},
   "source": [
    "#### Plotting loss curves of our effnetb2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44006325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(effnetb2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf785244",
   "metadata": {},
   "source": [
    "#### Let's save our effnetb2 model\n",
    "\n",
    "We will be using the function from utils.save_model() function created in going_modular directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import utils\n",
    "\n",
    "# Save the model\n",
    "utils.save_model(model=effnetb2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92bd74",
   "metadata": {},
   "source": [
    "#### Checking the size of EffNetB2 feature extractor\n",
    "\n",
    "Since one of our criteria for deploying a model to power FoodVision Mini is **speed** (~30FPS or better), let's check the size of our model.\n",
    "\n",
    "Why check the size?\n",
    "\n",
    "Well, while not always the case, the size of a model can influence its inference speed.\n",
    "\n",
    "As in, if a model has more parameters, it generally performs more operations and each one of these operations requires some computing power.\n",
    "\n",
    "And because we'd like our model to work on devices with limited computing power (e.g. on a mobile device or in a web browser), generally, the smaller the size the better (as long as it still performs well in terms of accuracy).\n",
    "\n",
    "We will create function to do this job in the next code cell using Path.stat().st_size which returns in bytes, but we will convert it to megabytes by dividing it to 1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4fa942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size(path: str):\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Path to the saved model file\n",
    "    model_path = Path(path)\n",
    "\n",
    "    # Check if file exists and get size in bytes\n",
    "    if model_path.exists():\n",
    "        size_bytes = model_path.stat().st_size\n",
    "        size_mb = size_bytes / (1024 * 1024)\n",
    "        print(f\"Model size: {size_mb:.2f} MB\")\n",
    "        return size_mb\n",
    "    else:\n",
    "        print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113981e",
   "metadata": {},
   "source": [
    "#### Collecting EffNetB2 feature extractor stats\n",
    "\n",
    "We've got a few statistics about our EffNetB2 feature extractor model such as test loss, test accuracy and model size, how about we collect them all in a dictionary so we can compare them to the upcoming ViT feature extractor.\n",
    "\n",
    "And we'll calculate an extra one for fun, total number of parameters.\n",
    "\n",
    "We can do so by counting the number of elements (or patterns/weights) in `effnetb2.parameters()`. We'll access the number of elements in each parameter using the [`torch.numel()`](https://pytorch.org/docs/stable/generated/torch.numel.html) (short for \"number of elements\") method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_parameters(model: torch.nn.Module):\n",
    "    return sum(torch.numel(param) for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb29431",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n",
    "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
    "                  \"number_of_parameters\": model_parameters(effnetb2),\n",
    "                  \"model_size (MB)\": model_size(\"models/pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")}\n",
    "effnetb2_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4550f",
   "metadata": {},
   "source": [
    "### Creating ViT feature extractor model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82577d",
   "metadata": {},
   "source": [
    "#### Functionizing creating ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_model(num_classes:int=3, \n",
    "                     seed:int=42):\n",
    "    # Create ViT_B_16 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights=weights)\n",
    "\n",
    "    # Freeze all layers in model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head to suit our needs (this will be trainable)\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
    "                                          out_features=num_classes)) # update to reflect target number of classes\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit, vit_transforms = create_vit_model(num_classes=3,\n",
    "                                       seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce4d53",
   "metadata": {},
   "source": [
    "#### Create DataLoaders for our ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import data_setup\n",
    "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloader(train_dir=train_dir,\n",
    "                                                                                       test_dir=test_dir,\n",
    "                                                                                       transform=vit_transforms,\n",
    "                                                                                       batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b76037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "# Setup loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train ViT model with seeds set for reproducibility\n",
    "set_seeds()\n",
    "vit_results = engine.train(model=vit,\n",
    "                           train_dataloader=train_dataloader_vit,\n",
    "                           test_dataloader=test_dataloader_vit,\n",
    "                           epochs=10,\n",
    "                           optimizer=optimizer,\n",
    "                           loss_fn=loss_fn,\n",
    "                           device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8323b481",
   "metadata": {},
   "source": [
    "#### Plotting loss curve for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6181dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(vit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a42662",
   "metadata": {},
   "source": [
    "#### Saving our ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import utils\n",
    "\n",
    "utils.save_model(model=vit,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606e332",
   "metadata": {},
   "source": [
    "#### Create ViT stats dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ViT statistics dictionary\n",
    "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
    "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
    "             \"number_of_parameters\": model_parameters(vit),\n",
    "             \"model_size (MB)\": model_size(\"models/pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")}\n",
    "\n",
    "vit_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cbfba1",
   "metadata": {},
   "source": [
    "### Making predictions with our trained models and timing them\n",
    "\n",
    "We've got a couple of trained models, both performing pretty well.\n",
    "\n",
    "Now how about we test them out doing what we'd like them to do?\n",
    "\n",
    "As in, let's see how they go making predictions (performing inference).\n",
    "\n",
    "We know both of our models are performing at over 95% accuracy on the test dataset, but how fast are they?\n",
    "\n",
    "To find out how long each of our models take to performance inference, let's create a function called `pred_and_store()` to iterate over each of the test dataset images one by one and perform a prediction. \n",
    "\n",
    "We'll time each of the predictions as well as store the results in a common prediction format: a list of dictionaries (where each element in the list is a single prediction and each sinlge prediction is a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "test_data_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from timeit import default_timer as timer \n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "def pred_and_store(model: torch.nn.Module,\n",
    "                   paths: List[pathlib.Path],\n",
    "                   transform: torchvision.transforms,\n",
    "                   class_names:List[str],\n",
    "                   device: torch.device):\n",
    "    #create a list to store dictionaries\n",
    "    pred_list=[]\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        pred_dict={}\n",
    "        pred_dict[\"image_path\"] = path\n",
    "        class_name = path.parent.stem\n",
    "        pred_dict[\"class_name\"] = class_name\n",
    "\n",
    "        start_time=timer()\n",
    "\n",
    "        img = Image.open(path)\n",
    "\n",
    "\n",
    "        img = Image.open(path)\n",
    "\n",
    "        transformed_img=transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            pred_logit=model(transformed_img)\n",
    "            pred_prob=torch.softmax(pred_logit, dim=1)\n",
    "            pred_label=torch.argmax(pred_prob, dim=1)\n",
    "            pred_class=class_names[pred_label.cpu()]\n",
    "\n",
    "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
    "            pred_dict[\"pred_class\"] = pred_class\n",
    "\n",
    "            end_time = timer()\n",
    "            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
    "        pred_dict[\"correct\"] = class_name == pred_class\n",
    "\n",
    "        pred_list.append(pred_dict)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b342c87",
   "metadata": {},
   "source": [
    "#### Making predictions and analyzing it for our effnetb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                          model=effnetb2,\n",
    "                                          transform=effnetb2_transforms,\n",
    "                                          class_names=class_names,\n",
    "                                          device=\"cpu\") # make predictions on CPU \n",
    "\n",
    "effnetb2_test_pred_dicts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn predicts into dataframe\n",
    "import pandas as pd\n",
    "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
    "effnetb2_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
    "effnetb2_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5700b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
    "                                     model=vit,\n",
    "                                     transform=vit_transforms,\n",
    "                                     class_names=class_names,\n",
    "                                     device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c831592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn vit_test_pred_dicts into a DataFrame\n",
    "import pandas as pd\n",
    "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
    "vit_test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5cbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_test_pred_df.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42629956",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
    "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
    "vit_average_time_per_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61872d6c",
   "metadata": {},
   "source": [
    "#### Comparing 2 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5202ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
    "\n",
    "# Add column for model names\n",
    "df[\"model\"] = [\"EffNetB2\", \"ViT\"]\n",
    "\n",
    "# Convert accuracy to percentages\n",
    "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7635cde",
   "metadata": {},
   "source": [
    "## Bringing FoodVisionMini to life by creating a Gradio demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7766dc",
   "metadata": {},
   "source": [
    "### Creting function to map our inputs to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b96c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38696c6b",
   "metadata": {},
   "source": [
    "#### Passing image to check our predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e003b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Get a list of all test image filepaths\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "\n",
    "# Randomly select a test image path\n",
    "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
    "\n",
    "# Open the target image\n",
    "image = Image.open(random_image_path)\n",
    "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
    "\n",
    "# Predict on the target image and print out the outputs\n",
    "pred_dict, pred_time = predict(img=image)\n",
    "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
    "print(f\"Prediction time: {pred_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf763814",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895489ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create title, description and article strings\n",
    "title = \"FoodVision Mini ðŸ•ðŸ¥©ðŸ£\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created by Makhmud Kholmuminov\"\n",
    "\n",
    "# Create the Gradio demo\n",
    "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "# Launch the demo!\n",
    "demo.launch(debug=False, # print errors locally?\n",
    "            share=True) # generate a publically shareable URL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0733de",
   "metadata": {},
   "source": [
    "## Turning Gradio demo to a deployable app\n",
    "\n",
    "We've seen our FoodVision Mini model come to life through a Gradio demo.\n",
    "\n",
    "But what if we wanted to share it with our friends?\n",
    "\n",
    "Well, we could use the provided Gradio link, however, the shared link only lasts for 72-hours.\n",
    "\n",
    "To make our FoodVision Mini demo more permanent, we can package it into an app and upload it to [Hugging Face Spaces](https://huggingface.co/spaces/launch).\n",
    "\n",
    "Hugging Face Spaces is a resource that allows you to host and share machine learning apps.\n",
    "\n",
    "Building a demo is one of the best ways to showcase and test what you've done.\n",
    "\n",
    "And Spaces allows you to do just that.\n",
    "\n",
    "You can think of Hugging Face as the GitHub of machine learning.\n",
    "\n",
    "If having a good GitHub portfolio showcases your coding abilities, having a good Hugging Face portfolio can showcase your machine learning abilities.\n",
    "\n",
    "> **Note:** There are many other places we could upload and host our Gradio app such as, Google Cloud, AWS (Amazon Web Services) or other cloud vendors, however, we're going to use Hugging Face Spaces due to the ease of use and wide adoption by the machine learning community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c87c39",
   "metadata": {},
   "source": [
    "### Creating a file structure of our app\n",
    "\n",
    "```\n",
    "demos/\n",
    "â””â”€â”€ foodvision_mini/\n",
    "    â”œâ”€â”€ pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
    "    â”œâ”€â”€ app.py\n",
    "    â”œâ”€â”€ examples/\n",
    "    â”‚   â”œâ”€â”€ example_1.jpg\n",
    "    â”‚   â”œâ”€â”€ example_2.jpg\n",
    "    â”‚   â””â”€â”€ example_3.jpg\n",
    "    â”œâ”€â”€ model.py\n",
    "    â””â”€â”€ requirements.txt\n",
    "```\n",
    "\n",
    "Where:\n",
    "* `pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth` is our trained PyTorch model file.\n",
    "* `app.py` contains our Gradio app (similar to the code that launched the app).\n",
    "    * **Note:** `app.py` is the default filename used for Hugging Face Spaces, if you deploy your app there, Spaces will by default look for a file called `app.py` to run. This is changeable in settings.\n",
    "* `examples/` contains example images to use with our Gradio app.\n",
    "* `model.py` contains the model definition as well as any transforms associated with the model.\n",
    "* `requirements.txt` contains the dependencies to run our app such as `torch`, `torchvision` and `gradio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create FoodVision mini demo path\n",
    "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
    "\n",
    "# Remove files that might already exist there and create new directory\n",
    "if foodvision_mini_demo_path.exists():\n",
    "    shutil.rmtree(foodvision_mini_demo_path)\n",
    "# If the file doesn't exist, create it anyway\n",
    "foodvision_mini_demo_path.mkdir(parents=True, \n",
    "                                exist_ok=True)\n",
    "    \n",
    "# Check what's in the folder\n",
    "!ls demos/foodvision_mini/\n",
    "\n",
    "#Create an examples directory\n",
    "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
    "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Collect three random test dataset image paths\n",
    "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
    "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
    "\n",
    "#Copy the three random images to the examples directory\n",
    "for example in foodvision_mini_examples:\n",
    "    destination = foodvision_mini_examples_path / example.name\n",
    "    print(f\"[INFO] Copying {example} to {destination}\")\n",
    "    shutil.copy2(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148160d",
   "metadata": {},
   "source": [
    "#### Moving EffNetB2 model to our file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create a source path for our target model\n",
    "effnetb2_foodvision_mini_model_path = \"models/pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
    "\n",
    "# Create a destination path for our target model \n",
    "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
    "\n",
    "try:\n",
    "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
    "    \n",
    "    shutil.move(src=effnetb2_foodvision_mini_model_path, \n",
    "                dst=effnetb2_foodvision_mini_model_destination)\n",
    "    \n",
    "    print(f\"[INFO] Model move complete.\")\n",
    "except:\n",
    "    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
    "    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5a10e",
   "metadata": {},
   "source": [
    "#### Turning our function to create EffNetB2 to python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, \n",
    "                          seed:int=42):\n",
    "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): number of classes in the classifier head. \n",
    "            Defaults to 3.\n",
    "        seed (int, optional): random seed value. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): EffNetB2 feature extractor model. \n",
    "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
    "    \"\"\"\n",
    "    # Create EffNetB2 pretrained weights, transforms and model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Change classifier head with random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728eb0d",
   "metadata": {},
   "source": [
    "#### Creating app.py into our folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/app.py\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
    "\n",
    "### Model and transforms preparation\n",
    "\n",
    "\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
    "    num_classes=len(class_names)\n",
    ")\n",
    "\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
    "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
    "    )\n",
    ")\n",
    "\n",
    "### Predict function\n",
    "\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
    "    \"\"\"\n",
    "    start_time = timer()\n",
    "    \n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "# Gradio app\n",
    "\n",
    "# Create title, description and article strings\n",
    "title = \"FoodVision Mini ðŸ•ðŸ¥©ðŸ£\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
    "article = \"Created by Makhmud Kholmuminov\"\n",
    "\n",
    "# Create examples list from \"examples/\" directory\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "# Create the Gradio demo\n",
    "demo = gr.Interface(fn=predict,\n",
    "                    inputs=gr.Image(type=\"pil\"),\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), \n",
    "                             gr.Number(label=\"Prediction time (s)\")], \n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994b705",
   "metadata": {},
   "source": [
    "#### Writing requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile demos/foodvision_mini/requirements.txt\n",
    "torch>=2.5\n",
    "torchvision>=0.20\n",
    "gradio>=4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d055119",
   "metadata": {},
   "source": [
    "### Deploying our FoodVisionMini app to HuggingFace Spaces\n",
    "\n",
    "We've got a file containing our FoodVision Mini demo, now how do we get it to run on Hugging Face Spaces?\n",
    "\n",
    "There are two main options for uploading to a Hugging Face Space (also called a [Hugging Face Repository](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories), similar to a git repository): \n",
    "1. [Uploading via the Hugging Face Web interface (easiest)](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui).\n",
    "2. [Uploading via the command line or terminal](https://huggingface.co/docs/hub/repositories-getting-started#terminal).\n",
    "\n",
    "We will be using the second option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Embed FoodVision Mini Gradio demo\n",
    "IFrame(src=\"https://huggingface.co/spaces/makhmudlp/foodvision_mini/+\", width=900, height=750)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
